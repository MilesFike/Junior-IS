% This is a template for your written document.
%
% To compile using latexmk on the command line, run the following: 
% latexmk -pdf main.tex

\documentclass[12pt]{article}
\usepackage{setspace}
\singlespace
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{graphicx}
\title{\textbf{Project Topic}}
\author{Miles Fike}

\begin{document}

\maketitle

\section{Introduction}
Currently many different computer vision systems are used to interpret handwriting as digital text, but these systems are constrained usually to one style or lack the complexity to comprehend more challenging handwriting. The goal of this project is to create a series of computer vision systems that maintain accuracy while circumventing some of these constraints by having an AI redirect the given image of text to the appropriate computer vision system to accurately interpret it. By providing a single interface able to interpret and return text to users from diverse styles, a functional multipurpose AI system implementing computer vision systems has been created that can be implemented for many different purposes from a single source. This approach allows for greater adaptability and flexibility, as it enables the AI to recognize a wide variety of handwriting styles without being limited to a single computer vision system. The system is designed to evolve over time, allowing for the continual input of new computer vision systems to expand the AI models capabilities in reading and transcribing different kinds of text. In addition, by integrating multiple specialized vision modules, the AI can effectively balance precision while also ensuring that even uncommon or low quality handwriting can be interpreted with a reasonable degree of accuracy. 

For the implementation of this project, a simple application was created allowing image and optional text input. The image input will take the image of the text while the optional text box would allow for a description of the writing style, skipping the optional analysis from a large AI model to detect a specific writing style. From here, the file will be run through the required computer vision system or systems to convert it to plain text by an MCP Client sending the image to the MCP server for the determined writing style. This will be returned to the user in the form of a text file that they can download and if the size permits, it will also be displayed onto the page itself. 

The images chosen are diverse Latin alphabet handwriting styles including the generic modern handwriting found in the EMNIST data sets. The images of this text are converted using the PIL library to a similar size to that of the individual image files in the EMNIST database. From here, if they are a part of a larger or more popular writing style, the images will be fed into a convolutional neural network module which will perform the training and measure accuracy as it goes. If the accuracy is satisfactory, the resulting computer vision system will be made accessible to the AI implemented by MCP. Small fluctuations in accuracy do occur in each run of the program. If the text is from a smaller dataset then the necessary labelling is performed manually and text processing is attempted with the unmonitored clustering method. This provides far less accuracy, but the system still provides an output. This involves the same image processing steps as convolutional neural networks and also may be used by the AI. The text is output on the simple user interface in the same manner regardless of whichever computer vision system the AI chose to use.

By connecting these systems through the Model Context Protocol, this project demonstrates how new AI architectures can expand beyond the limited capabilities of individual datasets and models to form expandable systems for complex computer vision tasks. Future developments may include the implementation of more complex output with potential areas of inaccuracy, such as typos analyzed and identified by a program linked to a dictionary dataset, and the continuous integration of more and more computer vision systems.
\section{Related Works}
\subsection*{Computer Vision Methods}
Within the AI field of computer vision, one popular goal is the transcription of handwritten text. By different organizations, this is called handwriting recognition technology or Handwritten Text Recognition (HTR). Researchers implemented a convolutional neural network system was utilized to build a computer vision system able to recognize digits from the MNIST and achieve accuracy of up to 100\% on the number 1 in the test data\cite{10.1145/3727648.3727679}. Essentially a convolutional neural network uses a combination of a single input layer then convolution layers that create feature maps, pooling layers that decrease data, and fully connected layers to create and process feature maps from images and fully connected layers that link the neurons to each of the potential final output results\cite{10.1145/3727648.3727679}. The created convolutional neural network is able to receive further input images and can assign a label to them based on what it interprets the image to be. These are very effective systems when a large, thoroughly labeled, dataset is available, but according to the research focused on obscure texts, specifically a medieval book with a Runic script known as the \emph{Codex Runicus}, there are issues with methods like convolutional neural networks, and recurrent neural networks because they require deep learning that involves these vast labeled datasets inaccessible to some less common scripts and styles\cite{10.1145/3519306}. Alternative options that did not involve deep learning were studied and implemented to accurately transcribe segments of the \emph{Codex Runicus}. While doing this, they used learning free methods such as unsupervised clustering which creates clusters which are subdivided, then the labels of each cluster are propagated through the other symbols, and the few-shot classification method which seeks to represent each individual symbol as a node in a graph and compare similarity between each pair of symbols\cite{10.1145/3519306}. With modern methods, it is very possible to perform highly accurate machine learning for popular fonts and styles but the pursuit of handwritten text recognition for any obscure text may result in lower accuracy systems that avoid machine learning methods. These methods may all be used to accomplish similar tasks but at different scales, when attempting to transcribe a frequently used writing style with a large, labeled database available. a convolutional neural network should always be used. 

\subsection*{Datasets}

There are several different frequently implemented datasets for handwritten text recognition computer vision. As was seen in the implementation of a high accuracy convolutional neural network, the MNIST is a dataset used in computer vision systems\cite{10.1145/3727648.3727679}. This is a popular data set used for testing computer vision systems provided by the National Institute of Standards and Technology consisting of 70,000 grayscale images of 28 by 28 pixel digits\cite{10.1145/3727648.3727679}. Because the text is labelled accurately and organized, it allows for easy implementation of computer vision systems, but to add further data to a dataset like this or use the output computer vision system  to analyze another image, it would have to be processed similarly, a task that is much more complicated. On the NIST website for another dataset, the EMNIST, documentation clarifies that  this new data set includes images of characters from a-z in the same grayscale 28 by 28 pixel format used by the MNIST dataset\cite{7966217}. Each character in the dataset is labelled with its ASCII value. This makes many methods described for the other computer vision system very easily applicable to this extended version of the dataset they had used. More broadly, datasets are accessible for many written styles and languages, and complications only seem to arise when pursuing data sets for languages and styles that are rarely used.

The preparation of images for computer vision is a complex process involving simplifying images and often storing them on a gray scale. As was seen with the EMNIST images, they were represented in gray scale with gray scale pixels. Additionally, a gaussian blur smoothed the image, ROI Extraction Centered Frame, and resizing and resampling were applied to ensure that the images were the correct consistent size and had the proper shading\cite{7966217}. The final result was images with white text with pixels blurred into a black background along the edges of the letters. In implementations of interactive image measurement software using Tkinter and PIL libraries from Python, researchers showed how tools may be used to perform the individual tasks implemented for the EMNIST database\cite{10.1145/3727648.3727740}. They explain that the Python PIL libraries may be used to perform image processing such as grayscale conversion, Gaussian blur, image sharpening, binarization and more\cite{10.1145/3727648.3727740}. The PIL package in Python could effectively be used for the preparation of images for EMNIST computer vision systems. The same methods could also be easily implemented to feed images into the output computer vision systems during their implementation.
\subsection*{Model Context Protocol}
To connect AI models with the tools that they require to function optimally and perform all tasks that may be required of them, there are several different popular methods. One recently released was the Model Context Protocol (MCP) and according to its own website, this protocol is a standardized way to connect AI applications to other systems\cite{modelContextProtocol/getting-started/intro}. This is primarily described in the context of agentic AI applications. The Model Context Protocol simplifies the integration of algorithm models, platform tools, and data sources to an AI agents\cite{10.1145/3745238.3745531}. Using the model context protocol, AI systems would be able to access and use the computer vision systems that are created using different datasets. It would become the AI's task to select the appropriate computer vision system to analyze an image that is input into it.

\subsection*{Claude}
Claude is a large language model developed by Anthropic. Claude Opus 4.5, released in 2025, is capable of making tool calls through the Model Context Protocol\cite{ClaudeOpus4_5}. When configured as an MCP client, Claude can receive tool descriptions from connected MCP servers and determine which tool is most appropriate for a given task based on natural language context. This makes Claude suitable for serving as the intelligent decision-making component in systems that require selecting between multiple specialized tools.   




\section{Methodology}
While seeking to implement the necessary computer vision systems to complete this project, the computer vision systems frequently used by other researchers in the field were first explored. It was discovered that among them one of the most popular was the MNIST\cite{10.1145/3727648.3727679}. From there, further research soon revealed the existence of the EMNIST which included the full set of characters required for this project. Thus, the dataset's parameters were used to create a convolutional neural network (CNN), a multi-layer architecture designed for image classification tasks. The Python PyTorch library was used for all machine learning tasks. The model begins with an input of a single 28 by 28 pixel image. A kernel, which is a small sliding window that scans across the image to detect patterns, is applied with 10 filters of 5 by 5 size and a stride of 1, meaning the window moves one pixel at a time. This operation extracts 10 feature maps, which are processed versions of the image that highlight specific patterns like edges or curves. A pooling layer then halves the dimensions of each feature map to 12 by 12 by taking the maximum value in each region. Next, another convolutional layer expands the feature map depth to 20 using the same kernel size and stride. From here, a second pooling layer simplifies the feature map to dimensions of 4 by 4 for each map. Finally, three linear fully connected layers map the 320 inputs to 104 neurons, then reduce that to 52, and finally bring that down to 26, fully aligned with the number of potential characters in the dataset. This method was effective, achieving accuracies between 84\% and 86\% across several attempts. However, 86\% accuracy means many words would still be misspelled. For example, inputting the author's name (Figure 1) produced "ailksfike" instead of "milesfike." Because the goal is accurate transcription, the system was rewritten several times to improve results.

    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{imgs/milesFike.png}
        \caption{Miles Fike Name}
        \label{fig:1}
    \end{figure}

Next, the program was rewritten to utilize more convolutional frames. In the second attempt, a convolutional layer with 32 output channels produced 32 feature maps. The same pooling layer halved the dimensions of each feature map. The second convolutional layer output 64 channels, and the following pooling layer compressed them. From here, 3 fully connected layers took the 1024 outputs and connected them to 256 neurons, then to 104, and finally to the 26 output classes. This system, referred to as Model2, achieved accuracy of up to 89\% with no accuracy below 88\% in 4 separate trainings. Outputs from Figure 1 include "ailesfike", "nilrsfike", and "hilesfihe."

From here, another system was created starting with 100 feature maps. This used the same pooling layers and eventually attained 200 4 by 4 feature maps. The 3200 outputs were connected through fully connected layers to 208 neurons, then 104, and finally 26. This system, referred to as Model3, achieved 90\% accuracy consistently. Example output from Figure 1 includes "rilesfike." With only one letter off, Model3 performs similarly to Model2 but with significantly longer run time, indicating that Model2's architecture may have been sufficient.

A fourth model was created using the same convolutional and pooling layers as Model3 but with 6 fully connected layers: 3200 to 1600, then 800, 410, 208, 104, and finally 26. This resulted in 89\% or 90\% accuracy. The lack of improvement shows that the fundamental issue is likely the amount of training data, not the network architecture. Because Model3 was optimal, it was exported to the MCP server.

To account for uppercase letters, lowercase letters, and digits, a separate system was developed using the EMNIST ByClass dataset with 62 character classes. The architecture follows Model2 but with adjusted output dimensions. The first convolutional layer applies 32 filters with a 5 by 5 kernel and stride of 1, producing feature maps of dimension $(28-5)+1=24$, reduced to 12 by 12 through max pooling. A second convolutional layer expands to 64 channels, resulting in $(12-5)+1=8$ dimension feature maps, compressed to 4 by 4 through pooling. The 1024 outputs ($64 \times 4 \times 4$) pass through three fully connected layers: 256 neurons, then 104, and finally 62 classes. The ReLU activation function converts negative values to zero after each convolutional layer, while hyperbolic tangent (tanh) normalizes outputs between -1 and 1 between fully connected layers. All EMNIST-based models were trained for 2 epochs, as the large dataset allowed adequate accuracy quickly. This system had longer training time due to the 62 classes but was ultimately implemented in the MCP Server because it handles capitalization and distinguishes between digits and letters.

The full EMNIST was broken into training and testing sets using its own built in attributes. This will be explained further in future drafts.

Loss measures how confident the system is in its predictions. It is derived from the system's perceived likelihood that a given image matches a given character. As the system trains, loss changes with each image processed. Ideally, loss should be 0, indicating 100\% certainty. Model3's loss is seen below

There will be a graph for the loss of Model3 here, but this proves difficult to construct as there are so many data points. The program will be altered to get averages over a wider area before making this graph.

As the graph indicates, in the first several hundred images processed, loss decreased drastically indicating rapid development by the system. However, in the final hundreds of images loss fluctuated around 0.3 and does not as consistently decrease. This indicates that the system may require substantially more training data to improve its accuracy further.

For image preparation, each letter needs to be separated from the larger image of text. The first approach identified isolated blobs of white pixels by iterating through surrounding pixels. This proved ineffective for several reasons. The system picked up background noise like smudges and tried to label them as letters. A Gaussian blur, which smooths the image by averaging nearby pixels, could prevent this issue. The real problem appeared with "i" and "j." The dots on these letters were identified as separate characters, making proper isolation impossible. Looking for nearby images did not help because the dots were often significantly separated from the letter bodies. 

Because of these flaws, a separate program was developed. The program converts the image to grayscale, then uses OpenCV's threshold function to convert it to black and white. Any pixel that is not absolutely black is made white. The image is iterated through by row to separate each horizontal line of text, stored as separate PNG files. Each line is then iterated through by column. If a column contains a pixel, it is marked. When columns no longer contain pixels, that is marked as well. These column boundaries are used to crop each character from the original image. The resulting images tend to have unusual proportions and tall heights.

A separate Python module processes individual letter images for the computer vision system. Characters are blurred using a Gaussian blur to smooth colors and reduce background noise. A threshold converts pixels with color greater than 100 (on a 0-255 scale) to black. Black and white pixels are then swapped to create white text on a black background. Images are padded into perfect squares to prevent warping during resizing, then converted to 128 by 128 pixels. The bounding box of the character is identified and the image is cropped with 2 pixel padding before being resized to 28 by 28 pixels. Each image is mirrored and rotated 90 degrees to match the EMNIST dataset orientation. The motivation behind this orientation remains unclear. Labels are returned left to right from the original image to produce the final text output.

In addition to the EMNIST-based computer vision system, a separate system was developed using the CHoiCe (Characters of Historical Choice) dataset to demonstrate the expandability of the MCP architecture for handling alternative handwriting styles. The CHoiCe dataset is a significantly smaller collection of handwritten character images compared to the EMNIST, requiring a different approach to training. The same neural network architecture used for the EMNIST ByClass system was employed, utilizing 32 and 64 channel convolutional layers with identical pooling and fully connected layer structures outputting to 62 character classes. However, due to the limited size of the CHoiCe dataset, the training process required substantially more epochs to achieve reasonable accuracy.

The CHoiCe computer vision system was trained for 45 epochs, a dramatic increase compared to the 2 epochs used for the EMNIST models. This extended training period was necessary because smaller datasets require the model to iterate over the available data many more times to learn meaningful patterns. The dataset was split into 80\% training data and 20\% testing data using a random split. During training, the system used stochastic gradient descent with a learning rate of 0.01 and momentum of 0.9, with cross-entropy loss as the optimization criterion.

The accuracy results for the CHoiCe system demonstrated significant inconsistency and variability between training runs. When trained for 30 epochs, the system achieved accuracies of 61\% and 65\% across different runs. When the training was extended to 45 epochs, the accuracy measured at 60\%, which was actually lower than some of the 30-epoch results. This inconsistency shows the challenges of training neural networks on small datasets, where random weight initialization and the stochastic nature of the training process can lead to substantially different outcomes. The variability in results suggests that the limited training data makes the model sensitive to initial conditions and may cause overfitting in some runs while underfitting in others. The first 10 output classes corresponding to digits (0-9) were eventually excluded from the CHoiCe MCP implementation due to higher confusion rates between numeric and alphabetic characters, an adjustment made in an attempt to sustain higher accuracy for letter recognition. Despite these limitations, the CHoiCe computer vision system was successfully integrated into the MCP Server, demonstrating that additional handwriting recognition systems can be incorporated into the framework regardless of their individual accuracy levels.

The MCP elements of this project were developed by allowing an MCP client to connect to each computer vision system exported as a MCP server or tool. The client registers each individual computer vision system as a tool accessible for the MCP client allowing AI to fluidly interact with each computer vision system. The image processing is built in as a function call within the running of the system, ensuring that the program runs effectively. The LLM when prompted with an image file identifies which tool is best to interact with the image if any and sends a tool call in the form of a JSON file containing the name of the tool and its parameters. If it is correct in its assumptions, a text file is returned containing all of the words transcribed by the MCP system.

\subsection*{EMNIST MCP Server}

The EMNIST MCP Server was developed to handle general handwriting recognition for modern handwriting styles. This server loads the trained EMNIST.pth model weights and exposes two tools to the MCP client: recognize\_character and recognize\_line. The recognize\_character tool accepts an image of a single handwritten character and returns the predicted character along with a confidence score. The recognize\_line tool accepts an image containing a full line of handwritten text, automatically segments it into individual characters, and returns the complete recognized text string along with confidence scores for each character.

The server includes the full image preprocessing pipeline built directly into its functions. When an image is input, it is first converted to grayscale and a Gaussian blur is applied to reduce background noise. From here, a threshold is applied to convert pixels to black and white, and non-black pixels are converted to white to create white text on a black background. The image is then padded into a square, resized to 128 by 128 pixels, cropped to the bounding box of the character with 2 pixel padding, and finally resized to 28 by 28 pixels to match the EMNIST format. The image is mirrored and rotated 90 degrees to match the orientation of images in the EMNIST dataset. The server can accept both local file paths and HTTP/HTTPS URLs for input images.

The neural network architecture embedded in the server uses the same structure as the EMNIST ByClass computer vision system with 32 and 64 channel convolutional layers, max pooling layers, and three fully connected layers outputting to 62 character classes. The ReLU activation function is applied after convolutional layers and the hyperbolic tangent function is used between fully connected layers. The server runs the model in evaluation mode and uses CUDA if available, otherwise defaulting to CPU processing.

\subsection*{CHoiCe MCP Server}

The CHoiCe MCP Server was developed to handle cursive and historical handwriting styles using the CHoiCe dataset trained model. This server loads the trained CHOICE.pth model weights and exposes two tools: recognize\_cursive\_character and recognize\_cursive\_line. These tools function similarly to the EMNIST server tools but are optimized for cursive handwriting. The server explicitly informs users that it cannot recognize digits (0-9) and only recognizes letters (A-Z, a-z) due to the exclusion of digit classes from the training process.

The image preprocessing for the CHoiCe server differs slightly from the EMNIST server to better handle the characteristics of cursive handwriting. The server uses Otsu's method for automatic threshold detection which handles images with varying brightness levels more effectively than a fixed threshold. The image is converted to grayscale, binarized using Otsu's threshold, and inverted to create white text on a black background. From here, the bounding box of the character is identified and the image is cropped tightly to the character boundaries. The cropped image is padded to a square, resized to 28 by 28 pixels, and converted to grayscale format matching the CHoiCe dataset training format.

The CHoiCe server uses the same neural network architecture as the EMNIST server with 32 and 64 channel convolutional layers and three fully connected layers outputting to 62 classes. However, when a digit class (0-9) is predicted, the server may produce unreliable results since these classes were excluded from training. The server provides confidence scores for each prediction allowing users to assess the reliability of the recognition results.

The recognize\_cursive\_line tool has a significant limitation due to the nature of cursive handwriting. The line segmentation algorithm relies on identifying gaps between characters by scanning for columns of pixels that contain no white pixels. This works for print handwriting where characters are naturally separated, but cursive handwriting connects letters together in a continuous flow. Because of this, the segmentation algorithm cannot properly separate conjoined cursive characters. It will often interpret an entire word as a single character or incorrectly split characters at arbitrary points where strokes happen to thin. This makes the recognize\_cursive\_line tool only functional for cursive text where characters have already been separated, which is not particularly useful for most real-world cursive handwriting recognition scenarios. The tool does still allow multiple pre-separated cursive characters to be processed at once if circumstances requiring such arise, but this is a narrow use case. Developing a proper cursive segmentation algorithm proved extremely difficult. Cursive connections vary significantly between writers and even between different letter combinations within the same writer's handwriting. Methods such as analyzing stroke direction changes, identifying local minima in character width, and using machine learning for segmentation were considered but not implemented due to the complexity involved.

To interact with this system, Anthropic's Claude Opus 4.5 model serves as the MCP client. Claude Opus 4.5 is a large language model capable of making tool calls through the Model Context Protocol. When given an image path and information about the handwriting style, Claude determines which MCP server tool is most appropriate to use based on the tool descriptions provided by each server. Claude then sends a tool call in the form of a JSON request containing the tool name and image path parameter. The MCP server processes the image through the appropriate computer vision system and returns the recognized text to Claude, which then presents the results to the user. This approach allows Claude to intelligently select between the EMNIST server for modern print handwriting and the CHoiCe server for cursive or historical handwriting styles based on context provided by the user.

\section{Results}

For the results section, a graph of the loss of each computer vision system will be shown as the project progressed. This will be shown for earlier renditions of the system as well as the final chosen system. It will be used to demonstrate the accuracy of the system. This has not been done yet because it is believed that a lower loss from the system will still be attained.

Examples of lines of text and the output from Claude's tool calls will also be shown. This unfortunately does not exist yet.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{ProjectCode/imgs/l.jpg}
    \caption{A cursive lowercase "l" used to test both computer vision systems.}
    \label{fig:cursive_l}
\end{figure}

When both MCP servers were given an image of a cursive lowercase "l" shown in Figure \ref{fig:cursive_l}, the results demonstrated the challenges of handwriting recognition. The EMNIST server predicted an uppercase "I" with 29.4\% confidence. The CHoiCe server predicted a "D" with 96.77\% confidence. The intended character was a lowercase cursive "l" written in a round cursive style. This particular "l" was fundamentally flawed as input because the round cursive shape became distorted during image processing, making it appear more circular than the typical pipe-shaped lowercase "l" that OCR systems expect. The EMNIST server's low confidence guess of uppercase "I" shows the model recognized that the input did not match any character well. The CHoiCe server's high confidence prediction of "D" represents a false positive, where the model is very certain about an incorrect answer. This is a known issue with neural networks trained on small datasets where the model can become overconfident in its predictions.

As was previously shown in the example with figure1, the handwritten name input into the system, errors in transcription are frequent and often very obvious. Additionally, spaces may not be detected by the system. Among x attempts in running our convolutional neural network Model3 on Figure1, an average of y characters were seen incorrectly transcribed. This reflects the system’s overall accuracy but also shows that the system is far from perfect. Generally

The next section of the results will show how different text styles were successfully identified by the Model Context Protocol. This also does not exist yet but the MCP will be configured to return the name of the system that it used. 

\section{Conclusion}
This project demonstrates the potential of integrating multiple specialized computer vision systems into a single, adaptive AI framework for handwriting recognition. By redirecting input images to the most suitable model, the system overcomes the limitations of single computer vision system strategies. The design provides a foundation for expansion as new models are developed. The result is a flexible, scalable handwriting recognition system capable of evolving as more computer vision systems are added.

Several improvements could enhance this system in future development. First, the cursive segmentation algorithm could be improved by implementing machine learning-based approaches that learn to identify character boundaries from training data rather than relying on simple column-based gap detection. Recurrent neural networks or transformer architectures could potentially learn the patterns of cursive connections and identify where one character ends and another begins. Second, ligature recognition could be implemented to handle common letter combinations that are written as connected units. Ligatures like "ff", "fi", "fl", and "th" appear frequently in both cursive and historical handwriting, where adjacent letters merge into single glyphs. Training the system to recognize these combined forms rather than attempting to segment them into individual characters could improve accuracy for cursive and historical texts. Third, the CHoiCe model accuracy could be improved by data augmentation techniques such as rotating, scaling, and adding noise to training images to artificially expand the small dataset. Despite the poor accuracy results from the CHoiCe system, this work demonstrates the importance of specialized character recognition for different handwriting styles. A general-purpose model trained only on modern handwriting cannot adequately handle historical scripts, and the MCP architecture allows these specialized models to coexist and be selected appropriately. Fourth, a confidence-based fallback system could be implemented where if one model returns low confidence scores, the system automatically tries alternative models and compares results. Fifth, post-processing with dictionary lookup could correct obvious spelling errors by comparing recognized text against known words and suggesting corrections. Sixth, the system could be expanded to handle non-Latin scripts by training additional models on datasets for Cyrillic, Arabic, Chinese, and other writing systems. Seventh, support for historical fonts with different letter forms could be added. Many historical documents use letter forms that differ significantly from modern writing, such as the long s (ſ) that resembles an "f", blackletter scripts with angular letterforms, or early printing typefaces with distinct character shapes. Training specialized models to recognize these historical letter forms would enable transcription of a wider range of archival documents. Finally, a web-based interface could make the system more accessible by allowing users to upload images directly through a browser rather than requiring local file paths.
\bibliographystyle{acm}
\bibliography{bibliography.bib}

\end{document}
