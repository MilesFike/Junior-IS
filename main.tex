% This is a template for your written document.
%
% To compile using latexmk on the command line, run the following: 
% latexmk -pdf main.tex

\documentclass[12pt]{article}
\usepackage{setspace}
\singlespace
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{graphicx}
\title{\textbf{Project Topic}}
\author{Your Name}

\begin{document}

\maketitle

\section{Introduction}
Currently many different computer vision systems are used to interpret handwriting as digital text, but these systems are constrained usually to one style or lack the complexity to comprehend more challenging handwriting. My goal is to create a series of computer vision systems that maintain accuracy while circumventing some of these constraints by having an AI redirect the given image of text to the appropriate computer vision system to accurately interpret it. By providing a single interface able to interpret and return text to users from diverse styles, I have created a functional multipurpose AI system implementing computer vision systems that can be implemented for many different purposes from a single source. This approach allows for greater adaptability and flexibility, as it enables the AI to recognize a wide variety of handwriting styles without being limited to a single computer vision system. The system is designed to evolve over time, allowing for the continual input of new computer vision systems to expand the AI models capabilities in reading and transcribing different kinds of text. In addition, by integrating multiple specialized vision modules, the AI can effectively balance precision while also ensuring that even uncommon or low quality handwriting can be interpreted with a reasonable degree of accuracy. 
For the implementation of this project, I created a simple application allowing image and optional text input. The image input will take the image of the text while the optional text box would allow for a description of the writing style, skipping the optional analysis from a large AI model to detect a specific writing style. From here, the file will be run through the required computer vision system or systems to convert it to plain text by an MCP Client sending the image to the MCP server for the determined writing style. This will be returned to the user in the form of a text file that they can download and if the size permits, it will also be displayed onto the page itself. 
The images chosen are diverse Latin alphabet handwriting styles including the generic modern handwriting found in the EMNIST data sets. I convert the images of this text using the PIL library to convert it to a similar size to that of the individual image files in the EMNIST database. From here, if they are a part of a larger or more popular writing style, the images will be fed into a convolutional neural network module which will perform the training and measure accuracy as it goes. If the accuracy is satisfactory, the resulting computer vision system will be made accessible to the AI implemented by MCP. Small fluctuations in accuracy do occur in each run of the program. If the text is from a smaller dataset then I perform the necessary labelling myself and attempt to process text with the unmonitored clustering method. This provides far less accuracy, but the system is still provides an output. This involves the same image processing steps as convolutional neural networks and also may be used by the AI. The text is output on the simple user interface in the same manner regardless of whichever computer vision system the AI chose to use.
By connecting these systems through the Model Context Protocol, this project demonstrates how new AI architectures can expand beyond the limited capabilities of individual datasets and models to form expandable systems for complex computer vision tasks. Future developments may include the implementation of more complex output with potential areas of inaccuracy, such as typos analyzed and identified by a program linked to a dictionary dataset, and the continuous integration of more and more computer vision systems.
\section{Related Works}
\subsection*{Computer Vision Methods}
Within the AI field of computer vision, one popular goal is the transcription of handwritten text. By different organizations, this is called handwriting recognition technology or Handwritten Text Recognition (HTR). Researchers implemented a convolutional neural network system was utilized to build a computer vision system able to recognize digits from the MNIST and achieve accuracy of up to 100\% on the number 1 in the test data\cite{10.1145/3727648.3727679}. Essentially a convolutional neural network uses a combination of a single input layer then convolution layers that create feature maps, pooling layers that decrease data, and fully connected layers to create and process feature maps from images and fully connected layers that link the neurons to each of the potential final output results\cite{10.1145/3727648.3727679}. The created convolutional neural network is able to receive further input images and can assign a label to them based on what it interprets the image to be. These are very effective systems when a large, thoroughly labeled, dataset is available, but according to the research focused on obscure texts, specifically a medieval book with a Runic script known as the \emph{Codex Runicus}, there are issues with methods like convolutional neural networks, and recurrent neural networks because they require deep learning that involves these vast labeled datasets inaccessible to some less common scripts and styles\cite{10.1145/3519306}. Alternative options that did not involve deep learning were studied and implemented to accurately transcribe segments of the \emph{Codex Runicus}. While doing this, they used learning free methods such as unsupervised clustering which creates clusters which are subdivided, then the labels of each cluster are propagated through the other symbols, and the few-shot classification method which seeks to represent each individual symbol as a node in a graph and compare similarity between each pair of symbols\cite{10.1145/3519306}. With modern methods, it is very possible to perform highly accurate machine learning for popular fonts and styles but the pursuit of handwritten text recognition for any obscure text may result in lower accuracy systems that avoid machine learning methods. These methods may all be used to accomplish similar tasks but at different scales, when attempting to transcribe a frequently used writing style with a large, labeled database available. a convolutional neural network should always be used. 

\subsection*{Datasets}
There are several different frequently implemented datasets for handwritten text recognition computer vision. As was seen in the implementation of a high accuracy convolutional neural network, the MNIST is a dataset used in computer vision systems\cite{10.1145/3727648.3727679}. This is a popular data set used for testing computer vision systems provided by the National Institute of Standards and Technology consisting of 70,000 grayscale images of 28 by 28 pixel digits\cite{10.1145/3727648.3727679}. Because the text is labelled accurately and organized, it allows for easy implementation of computer vision systems, but to add further data to a dataset like this or use the output computer vision system  to analyze another image, it would have to be processed similarly, a task that is much more complicated. On the NIST website for another dataset, the EMNIST, documentation clarifies that  this new data set includes images of characters from a-z in the same grayscale 28 by 28 pixel format used by the MNIST dataset\cite{7966217}. Each character in the dataset is labelled with its ASCII value. This makes many methods described for the other computer vision system very easily applicable to this extended version of the dataset they had used. More broadly, datasets are accessible for many written styles and languages, and complications only seem to arise when pursuing data sets for languages and styles that are rarely used.
The preparation of images for computer vision is a complex process involving simplifying images and often storing them on a gray scale. As was seen with the EMNIST images, they were represented in gray scale with gray scale pixels. Additionally, a gaussian blur smoothed the image, ROI Extraction Centered Frame, and resizing and resampling were applied to ensure that the images were the correct consistent size and had the proper shading\cite{7966217}. The final result was images with white text with pixels blurred into a black background along the edges of the letters. In implementations of interactive image measurement software using Tkinter and PIL libraries from Python, researchers showed how tools may be used to perform the individual tasks implemented for the EMNIST database\cite{10.1145/3727648.3727740}. They explain that the Python PIL libraries may be used to perform image processing such as grayscale conversion, Gaussian blur, image sharpening, binarization and more\cite{10.1145/3727648.3727740}. The PIL package in Python could effectively be used for the preparation of images for EMNIST computer vision systems. The same methods could also be easily implemented to feed images into the output computer vision systems during their implementation.
\subsection*{Model Context Protocol}
To connect AI models with the tools that they require to function optimally and perform all tasks that may be required of them, there are several different popular methods. One recently released was the Model Context Protocol (MCP) and according to its own website, this protocol is a standardized way to connect AI applications to other systems\cite{modelContextProtocol/getting-started/intro}. This is primarily described in the context of agentic AI applications. The Model Context Protocol simplifies the integration of algorithm models, platform tools, and data sources to an AI agents\cite{10.1145/3745238.3745531}. Using the model context protocol, AI systems would be able to access and use the computer vision systems that are created using different datasets. It would become the AI’s task to select the appropriate computer vision system to analyze an image that is input into it.   




\section{Methodology}
While seeking to implement the necessary computer vision systems to complete this project, I first explored the computer vision systems frequently used by other researchers in the field. I discovered that among them one of the most popular was the MNIST\cite{10.1145/3727648.3727679}. From there, further research soon revealed the existence of the EMNIST which included the full set of characters that I required for my project. Thus, I used the dataset’s parameters to create a convolutional neural network (CNN), a multi-layer architecture designed for image classification tasks. To perform the machine learning tasks involved, I utilized the Python PyTorch library. The model begins with an input of a single-image, 28 by 28 pixels. From here, kerneling with 10 filters with a 5 by 5 size and stride of 1 are applied in the first convolutional layer. This operation extracts 10 feature maps with emphasized details of the image. From here, a pooling layer halves the dimensions of each feature map reducing them to 12 by 12 squares. Next, another convolutional layer expands the feature map depth to 20 using the same kernel size and stride. From here, a second pooling layer simplifies the feature map to dimensions of 4 by 4 for each map. Finally, three linear fully connected layers map the 320 inputs to 104 neurons, then reduce that to 52, and finally bring that down to 26, fully aligned with the number of potential characters in the dataset. This method was effective. Through three separate attempts at processing the image, I attained an accuracies between 84\% and 86\% through several attempts at running this code. This shows that the system was successfully recognizing many images, but an accuracy of 86 percent means that many words would still be very misspelled. For example, By inputting this image of the letters in my name, Figure:1, I received the following characters as the output: “ailksfike” This is very similar to the intended result but still imperfect. Because the intention of this project is to accurately transcribe text, the program should be expected to correctly identify all characters. I rewrote the system a few times.

    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{imgs/milesFike.png}
        \caption{Miles Fike Name}
        \label{fig:1}
    \end{figure}

Next, I tried to rewrite the program to utilize more convolutional frames. In the second attempt, a convolutional layer with 32 output channels was used to iterate over the input layer and return 32 feature maps. From here I used the same pooling layer to halve the length and width of each individual feature map produced by the convolutional layer. The second convolutional layer output 64 different channels, and the following pooling layer compressed them. From here, 3 fully connected layers took the 1024 outputs and connected them to 256 neurons, then 256 neurons are connected to 104, and finally the 104 neurons are connected to the 26 aligned to each image. Due to the more extensive analysis of each input image by the increased number of feature maps detecting more details, this system, which will henceforth be referred to as Model2, had consistently higher accuracy. Model2 achieved accuracy of up to 89\% and no accuracy was recorded below 88% in 4 separate trainings of the system. Outputs for this system from figure1 include, “ailesfike”, “nilrsfike”, and “hilesfihe.”

From here, another system was created starting by filtering over the input layer for 100 feature maps. This used the same pooling layers and eventually attained 200 4 by 4 feature maps. The 3200 different outputs were connected to 208 neurons, then they were connected to 104, and finally, they were connected to just 26 by the fully connected layers. This system, henceforth referred to as Model3, usually resulted in a final accuracy of 90\% through every attempt at running it. Example outputs from the system using figure1 include, “rilesfike.” With only one letter off, it is similar to the outputs of the Model2. The lack of significant difference and significantly longer run time for this application indicates that the number of convolutional layers in Model 2 may have been enough.

A fourth and final model was created using the same convolutional and pooling layers as Model3, but with different fully connected layers. This model used 6 separate fully connected layers to link the neurons to the feature map. The first linked the 3200 outputs to 1600 neurons, then 1600 to 800, then 800 to 410, then 410 to 208, then 208 to 104, and finally 104 to 26. This resulted in an accuracy of 89\% or 90\%. This shows that the fundamental issue is likely the amount of training data input into our system. Because Model3 was the optimal system, it was the system that was exported to the MCP server.
I broke the full EMNIST into training and testing sets using its own built in attributes. This will be explained further in future drafts.

Loss is the indicator of a systems assumption of the correct character. It is derived from the system’s perceived percentage likelihood that a given image may be a given character. As the system is trained, the loss changes with each image it processes. Ideally, loss should be 0 indicating that the system is operating with 100\% certainty that the image input is a specific character. Model3’s loss is seen below

There will be a graph for the loss of Model3 here, but This proves difficult to construct as there are so many data points. I will actually alter the program to get averages over a wider area before making this graph.

As the graph indicates, in the first several hundred images processed, loss decreased drastically indicating rapid development by the system. However, in the final hundreds of images loss fluctuated around 0.3 and does not as consistently decrease. This indicates that the system may require substantially more training data to improve its accuracy further.

For the preparation of images, processing first needs to be applied to each individual letter that needs to be read by the system to separate each of them from the larger image of text input. To do this, the first program sought to separate each individual image of a letter by identifying isolated blobs of white pixels within the image of text. The program would identify a pixel and try to iterate through the surrounding pixels to identify if they were also part of the letter. This, however, proved ineffective for a few reasons. First of all, the system picked up noise in the background, such as smudges on paper, and tried to label it as letters. However, that could be prevented by applying a Gaussian blur that smoothed the image before black and white thresholding converted the image to two colors. The real issue appeared with the letters “i” and “j.” The dots on these letters were identified as separate letters by this method making it impossible to properly isolate the full “i” and it created an extra letter in the output. I could not just look for nearby images because often the dots of “i’s” were significantly separated from the body of the letter. 

Because of the flaws with this method, a separate program was prepared to read the complex. The program starts by converting the initial image into gray scale then using Opencv’s threshold function to convert the image to black and white. From here a black background was created for the image and any pixel that is not absolutely black was made white by iterating through the image. From here the image is iterated through by row to separate each horizontal line of text, which are stored as separate PNG files. Next, each PNG of a line of text is iterated through by each column of pixels. If there is a pixel in that column it is marked and when there are no longer any pixels that is marked as well. These sets of two columns are used to crop each separate each image from the original picture, not the grayed version. The resulting images tend to have unusual proportions and a tall height.

A separate Python module was made to process individual images of letters and to prepare them to be read by the computer vision system. This was applied iteratively to the results of the line reading function. From here, I took the characters and blurred the images using a Gaussian blur which smoothed the colors into transitions between black and white rather than just the two colors while also blurring out some background noise to produce a smoother overall picture. From here, a threshold was set that converted all pixels with a color greater than 100 on a 100 to 255 scale to black. Next, black pixels are converted to white and white pixels are converted to black to make the image white text on a black background. The images were converted into perfects squares by adding extra rows and columns to each side until the width and length were equal. This ensured that when the size of the images was decreased, the images would not be heavily compressed or altered. The images were then converted to 128 by 128 resulting in no apparent warping in any of my tests. Next, the images were iterated over to identify the highest, lowest, furthest left, and furthest right pixels. The image was reduced to just these pixels with two pixel padding before being padded back into a square before being further reduced once more into a 28 by 28-pixel square. To finish the image processing, I mirrored each individual image and rotated it 90 degrees to match the image orientation provided by the EMNIST in their data set. The motivation behind the orientation of these images remains unclear. The computer vision system from here assigned the appropriate labels to the images. The labels are returned in order from the left to right side of the original image to give the final text output. 

The MCP elements of this project were developed by allowing an MCP client to connect to each computer vision system exported as a MCP server or tool. The client registers each individual computer vision system as a tool accessible for the MCP client allowing AI to fluidly interact with each computer vision system. The image processing is built in as a function call within the running of the system, ensuring that the program runs effectively. The LLM when prompted with an image file identifies which tool is best to interact with the image if any and sends a tool call in the form of a JSON file containing the name of the tool and its parameters. If it is correct in its assumptions, a text file is returned containing all of the words transcribed by the MCP system.

To interact with this system, I built a basic Tkinter Graphical User Interface. The basic GUI contains two text boxes, one for the user to input the meta data for the image that they are adding and another for the LLM to return its transcription of the image. On the right side, there is a large drop box for images. It does not show the image input, but it does receive the files using another package tkinkterdnd2 which allows users to drag and drop images. The system stores the data input by the user and feeds it to the LLM’s prompt allowing for the function calls necessary to be determined by the system.

For each computer vision system within the database, I will describe how the required system was developed. I will also be able to more thoroughly and accurately describe the MCP once it actually does something.

\section{Results}

For the results section, I will show a graph of the loss of each computer vision system as I progressed. This will be shown for earlier renditions of the system as well as the final chosen system. It will be used to demonstrate the accuracy of the system. I have not done this yet because I believe that I will still attain a lower loss from the system

I will also show examples of lines of text and the output on the user interface. This unfortunately does not exist yet.

As was previously shown in the example with figure1, the handwritten name input into the system, errors in transcription are frequent and often very obvious. Additionally, spaces may not be detected by the system. Among x attempts in running our convolutional neural network Model3 on Figure1, an average of y characters were seen incorrectly transcribed. This reflects the system’s overall accuracy but also shows that the system is far from perfect. Generally

The next section of the results will show how different text styles were successfully identified by the Model Context Protocol. This also does not exist yet but I will force the MCP to return the name of the system that it used. 

\section{Conclusion}
In conclusion, this project demonstrates the potential of integrating multiple specialized computer vision systems into a single, adaptive AI framework for handwriting recognition. By redirecting input images to the most suitable model, the system effectively overcomes the limitations of single computer vision system strategies. This design not only enhances accuracy across diverse handwriting styles but also provides a foundation for expansion as new models are developed. The result is a flexible, scalable, and intelligent handwriting recognition system capable of evolving with future as more computer vision systems are added to it.

I will also include how each individual system compares to other computer vision systems that have already been released, and I will show how well different styles fare input into unprepared systems versus a system built to handle diverse styles.
\bibliographystyle{acm}
\bibliography{bibliography.bib}

\end{document}
