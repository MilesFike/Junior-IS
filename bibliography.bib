% Here is an example of how to create a bibliography entry for an article using
% BibTeX. Generally you won't have to write these out yourself, because they are
% provided by most web sites that allow you to export citations. The string
% "clrsAlgorithms" is a citation key, and if you were citing the source in a
% document you would use \cite{clrsAlgorithms}.
@inproceedings{10.1145/3727648.3727679,
  author    = {Yu, Ying and Tian, Yuhe},
  title     = {Research Application of Computer Vision-Based Convolutional Neural Network in Handwriting Recognition Technology},
  year      = {2025},
  isbn      = {9798400712647},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3727648.3727679},
  doi       = {10.1145/3727648.3727679},
  abstract  = {With the rapid development of artificial intelligence technology, handwriting recognition technology plays an important role in many fields. In this paper, the handwriting recognition technology based on convolutional neural network is studied in depth, the basic structure of convolutional neural network is introduced, the construction process of handwriting recognition system is described, and the effectiveness and superiority of the technology is verified through model training and experiments.},
  booktitle = {Proceedings of the 4th International Conference on Computer, Artificial Intelligence and Control Engineering},
  pages     = {177–181},
  numpages  = {5},
  keywords  = {Artificial Intelligence, Computer Vision, Convolutional Neural Networks, Handwriting Recognition},
  series    = {CAICE '25},
  annote    = {This paper analyzes how handwriting recognition technology based on convolution neural networks work. A convolution neural network is generally used for processing grid data and is a feed forward neural network. A convolutional neural network has an input layer that takes in data like 2D images, a convolution layer, which applies kernels to an image, which extract specific features to produce an output feature map, the pooling layer which is used to compress the data and prevent overfitting by taking the averages or maximums of regions, and next, the fully connected layer is used in the last layers of the network and perform tasks like classification on each feature, and finally, the output layer returns results. Handwriting recognition technology is used to convert handwriting into a computer-processable form. This handwriting recognition technology is more widely implemented now because of AI's development and used for diverse purposes including the preservation and retrieval of ancient books. The MNIST is a data set containing handwritten digits. Pytorch was used to build a handwriting recognition model with the MNIST as its training set. The resulting model was vastly successful, but had trouble with multiple digits in succession or with incoherent strokes. The authors of this source, Ying Yu and Yuhe Tian are both associated with the College of Design and Art Shenyang Architecture University, and Yuhe Tian has researched AI in the past, but it is difficult to determine the extent of their background in AI. The sources that the authors use were written by research scholars and faculty at other universities. Overall, the source seems reputable, but the authors are not easily researchable. This source's overview of computer vision implemented in the context of handwriting provided strong background information for any potential investigation computer vision's implementation in identifying historic ligatures which could prove challenging because the authors implied that the computer vision struggled with numbers in succession and this certainly would apply to letters as well. }
}

@article{10.1145/3687310,
  author     = {Ha, Soonhoi and Jeong, Eunjin},
  title      = {Software Optimization and Design Methodology for Low Power Computer Vision Systems},
  year       = {2024},
  issue_date = {January 2025},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {24},
  number     = {1},
  issn       = {1539-9087},
  url        = {https://doi.org/10.1145/3687310},
  doi        = {10.1145/3687310},
  abstract   = {This tutorial article addresses a low power computer vision system as an example of a growing application domain of neural networks, exploring various technologies developed to enhance accuracy within the resource and performance constraints imposed by the hardware platform. Focused on a given hardware platform and network model, software optimization techniques, including pruning, quantization, low-rank approximation, and parallelization, aim to satisfy resource and performance constraints while minimizing accuracy loss. Due to the interdependence of model compression approaches, their systematic application is crucial, as evidenced by winning solutions in the Lower Power Image Recognition Challenge (LPIRC) of 2017 and 2018. Recognizing the typical heterogeneity of processing elements in contemporary hardware platforms, the effective utilization through parallelizing neural networks emerges as increasingly vital for performance enhancement. The article advocates for a more impactful strategy—designing a network architecture tailored to a specific hardware platform. For detailed information on each technique, the article provides corresponding references.},
  journal    = {ACM Transactions on Embedded Computing Systems},
  month      = dec,
  articleno  = {19},
  numpages   = {31},
  keywords   = {Optimization, neural architecture search, parallelization, embedded machine learning},
  annote     = {This article written by Soonhoi Ha, a professor of Computer Science and Engineering, and Eunjin Jeong, a post-doctoral researcher in the same field, in 2024 discusses computer vision systems designed both for accuracy and system constraints. It encourages the use of specialized hardware to optimize computer vision systems with deep learning. They suggest reducing redundancy in deep learning with approximate computing that does not cause significant accuracy loss. Examples in this article demonstrate systems with several convolutional layers followed by a few fully connected layers. The article goes on to discuss the results of implementing different types of quantization, quantization being the reduction of bits in representation, pruning of unnecessary parameters, reduction of large-size kernels to reduce storage requirements and computation time, and other optimization techniques. The article also strongly emphasizes when different types of hardware such as GPUs can be used to process layers. This article has been downloaded hundreds of times and continues to be downloaded frequently but it has no recorded citations on the ACM Digital Library website. The sources the article references tend to originate from scientific journals and conferences and appear credible. The article provides specific code examples for how the elements of computer vision can be programmed which would be very applicable to work in the field, and it reinforces the understanding of the basic concepts of layers in computer vision by showing examples of their implementations.}
}

@inproceedings{10.1145/3544548.3580643,
  author    = {Gyory, Peter and Bae, S. Sandra and Yang, Ruhan and Do, Ellen Yi-Luen and Zheng, Clement},
  title     = {Marking Material Interactions with Computer Vision},
  year      = {2023},
  isbn      = {9781450394215},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3544548.3580643},
  doi       = {10.1145/3544548.3580643},
  abstract  = {The electronics-centered approach to physical computing presents challenges when designers build tangible interactive systems due to its inherent emphasis on circuitry and electronic components. To explore an alternative physical computing approach we have developed a computer vision (CV) based system that uses a webcam, computer, and printed fiducial markers to create functional tangible interfaces. Through a series of design studios, we probed how designers build tangible interfaces with this CV-driven approach. In this paper, we apply the annotated portfolio method to reflect on the fifteen outcomes from these studios. We observed that CV markers offer versatile materiality for tangible interactions, afford the use of democratic materials for interface construction, and engage designers in embodied debugging with their own vision as a proxy for CV. By sharing our insights, we inform other designers and educators who seek alternative ways to facilitate physical computing and tangible interaction design.},
  booktitle = {Proceedings of the Conference on Human Factors in Computing Systems},
  articleno = {478},
  numpages  = {17},
  keywords  = {Computer Vision, Making, Materiality, Physical Computing, Tangible Interactions},
  location  = {Hamburg, Germany},
  series    = {CHI '23'},
  annote    = {This paper, written in part by the faculty of the ATLAS Institute at the University of Colorado Boulder in 2023, describes a computer vision system that can be run on a local device with a webcam. This is especially helpful as previous articles have been dependent on hardware that may not be accessible to all. This source has 5 citations showing higher engagement than the other sources investigated. The researchers in this paper investigated the creation of a Tangible User Interface system relying on computer vision rather than electronics to receive input due to the struggles novices have with devices like Arduino. They developed the beholder JavaScript library which gives users control over a device’s cameras. Beholder could then use the cameras to read ArUco markers and acquire their metadata. ArUco markers resemble QR codes. After the library was designed, it was used in the projects of students. They would create systems and physical devices for which the ArUco marker could be read by Beholder, like a little arcade machine which moved the ArUco marker when a button was pressed. The system was easy to debug because visual information could indicate issues. Limitations of the system include proper lighting and space requirements for the camera. Aside from reinforcing a basic understanding of computer vision, this source is not the most relevant. However, this article does encourage consideration of elements like perspective and lightning when considering what a computer may be required to interpret.}
}

@article{10.1145/3754333,
  author    = {Ponte Ah\'{o}n, Santiago and Aidelman, Yael and Seery, Juan and Quiroga, Facundo Manuel and Ronchetti, Franco and Hasperu\'{e}, Waldo and Iannuzzi, Matilde and Peralta, Romina and Lopez, M\'{o}nica and Bariviera, Aurelio F. and Cidale, Lydia and Gamen, Roberto},
  title     = {{ReTrOH-UNLP}: Conservation of the Historical Observational Work of the Astronomical Observatory of La Plata with Computer Vision},
  year      = {2025},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  issn      = {1556-4673},
  url       = {https://doi.org/10.1145/3754333},
  doi       = {10.1145/3754333},
  abstract  = {The Observatorio Astron\'{o}mico de La Plata from the Facultad de Ciencias Astron\'{o}micas y Geof\'{\i}sicas (FCAG) of the Universidad Nacional de La Plata (UNLP) is one of the oldest observatories in South America. It has an enormous collection of observations, including thousands of spectroscopic and photographic glass plates. These observations were taken between the 1900s and the 1980s by renowned Argentine astronomers and constitute unique and unpublished records.Since 2019, the Recuperation of Historical Observational Work (ReTrOH, for its initials in Spanish) project has begun digitizing the 15,000+ samples of the spectroscopic plate collection.The digitization process for each plate is a complex, error-prone, multi-stage procedure requiring several person-hours. Consequently, a multidisciplinary team of astronomers and computer scientists was formed to develop the first system capable of assisting the user in this process. This system, designated PlateUNLP, employs signal processing algorithms and computer vision models to accelerate the process, reduce error rates, and standardizes data formats and processes. A key feature of PlateUNLP is its ability to automatically detect and record each spectrum recorded on the plates, along with the corresponding metadata, thus minimizing the need for user intervention.The developed system is publicly available under an open source license. Furthermore, while the system was developed with the ReTrOH project's requirements in mind, it could be adapted and used to assist with the digitization of thousands of other plates available around the world.PlateUNLP is therefore a first step towards making possible and practical the recovery of historical astronomical observations. Digitizing and exploiting these observations can provide a unique window into the past of our universe.},
  journal   = {J. Comput. Cult. Herit.},
  month     = jul,
  keywords  = {Astronomy, Spectrographic records, Spectroscopic plates, Computer Vision, YOLO, Object Detection, Wavelength calibration, Dynamic Time Warping},
  annote    = {This article describes the development of the PlateUNLP application which lowers the cost and time to digitize astronomical data in plate collections. This system improves upon the collection of this data by individuals, which is an error prone and time-consuming process. The application’s frontend was developed with HTML and JavaScript for a web app. A Node.js and JavaScript backend manages the storage digitized plate data storage. A deep-learning module for the detection of the object within each plate was developed with Flask and Python. PlateUNLP works by first receiving a scanned plate, then segmenting its observations, segmenting the three parts of each observation, converting the spectra from 2D to 1D, and finally the user assists in analyzing the data. The part of this process most relevant to the project is the segmentation of images. This projection created a YOLO (You Only Look Once) detection model which rather than running a classifier on regions of interests approaches segmentation as a regression problem. YOLO could be helpful for separating letters or lines of text. Additionally, the system allowed for changes in brightness, noise, vertical image flipping, rotation, and scaling to help interpret the image which could be important when using computer vision to identify letters and transcribe text if the letters are at odd angles or on problematic surfaces. The authors of this article were primarily students or researchers in Computer Science and Astronomy at The Universidad Nacional de La Plata in Buenos Aires, a university officially recognized by the Argentine Ministry of Education. It is difficult for me to thoroughly investigate each author, but all seemed to be very experienced within their fields.}
}

@inproceedings{10.1145/3745238.3745531,
  author    = {Zhang, Xueqiang and Dong, Xiaofei and Wang, Yiru and Zhang, Dan and Cao, Feng},
  title     = {A Survey of Multi-AI Agent Collaboration: Theories, Technologies and Applications},
  year      = {2025},
  isbn      = {9798400712791},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3745238.3745531},
  doi       = {10.1145/3745238.3745531},
  abstract  = {As an important application of large language model(LLM), artificial intelligence agent(AI Agent) have the ability to autonomously perceive, understand, plan, memory, act, and use tools. It can automate complex tasks and effectively empower various business scenarios. Single AI Agent flexible and diverse deployment, multi-AI Agent innovative interaction and collaboration, multi-AI Agent collaboration improves the autonomy of the intelligent system by integrating the capabilities of single AI Agent. This paper provides an overview of multi-AI Agent from four aspects. Firstly, the core capabilities of AI Agent were outlined, and multi-AI Agent collaboration was introduced and its characteristics were analyzed. Secondly, the theoretical basis, key technologies, and scenario applications of multi-AI Agent collaboration were discussed, and the mechanism, architecture design, communication protocol, reinforcement learning, security and trustworthiness of multi-AI Agent collaboration were deeply studied. Thirdly, the advantages and disadvantages of multi-AI Agent collaboration in technology, application, and security directions were summarized, and frontier research and innovation directions were provided. Finally, a summary and outlook were made on the high-quality development of multi-AI Agent collaboration.},
  booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence},
  pages     = {1875–1881},
  numpages  = {7},
  keywords  = {Collaborative mechanism, Communication protocol, Large language model, Multi-AI Agent collaboration, Reinforcement learning},
  location  = {
               },
  series    = {DEAI '25},
  annote    = {All authors for this journal article were researchers at the Nanjing Research Institute of Next-generation Artificial Intelligence or the China Academy of Information and Communications Technology. According to this article, AI agents convert LLMs to valuable tools with real application and problem-solving abilities. The memory of an AI agent allows it to plan how to best interact with the external environment by using function call techniques. With the multi-AI Agent collaboration system, different AI Agents can interact with each other. In the system, each AI Agent focuses on a specific domain or task and has specialized knowledge, skills, and experience in that area. Architecture can be vertical meaning that there is a lead agent or horizontal meaning all agents are equal. Agent network protocol and model context protocol are two protocols to help connect multiple agents. ANP has equal interaction between AI agents. MCP uses the model as its core and provides a common interface between all agents. AI agents may learn with technologies such as Multi agent reinforcement learning. This aims to teach optimal decision strategies through interactions with agents and their environment. Generally, multiple agent systems surpass the limitations of single agent systems, but may lack personalization, adaptability, and potentially coordination. Privacy issues are also present. This provides me with a fundamental understanding of the multiple agent interactions I wish to have controlling different computer vision systems. I may need to implement many computer vision systems to properly utilize the AI agents. This also provides very brief background on MCP.}
}

@article{10.1145/3519306,
  author     = {Souibgui, Mohamed Ali and Bensalah, Asma and Chen, Jialuo and Forn\'{e}s, Alicia and Waldisp\"{u}hl, Michelle},
  title      = {A User Perspective on HTR Methods for the Automatic Transcription of Rare Scripts: The Case of Codex Runicus},
  year       = {2023},
  issue_date = {December 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {15},
  number     = {4},
  issn       = {1556-4673},
  url        = {https://doi.org/10.1145/3519306},
  doi        = {10.1145/3519306},
  abstract   = {Recent breakthroughs in Artificial Intelligence, Deep Learning, and Document Image Analysis and Recognition have significantly eased the creation of digital libraries and the transcription of historical documents. However, for documents in rare scripts with few labelled training data available, current Handwritten Text Recognition (HTR) systems are too constraining. Moreover, research on HTR often focuses on technical aspects only, and rarely puts emphasis on implementing software tools for scholars in Humanities. In this article, we describe, compare, and analyse different transcription methods for rare scripts. We evaluate their performance in a real-use case of a medieval manuscript written in the runic script (Codex Runicus) and discuss advantages and disadvantages of each method from the user perspective. From this exhaustive analysis and comparison with a fully manual transcription, we raise conclusions and provide recommendations to scholars interested in using automatic transcription tools.},
  journal    = {J. Comput. Cult. Herit.},
  month      = mar,
  articleno  = {72},
  numpages   = {18},
  keywords   = {codex runicus, human evaluation, handwritten text recognition tools, Historical manuscripts},
  annote     = {The authors of this article include Mohamed Ali Souibgui, Asma Bensalah, Jialuo Chen, and Alicia Fornes, all of whom are researchers and engineers at the Computer Vision Center at the Autonomous University of Barcelona. They are all specialists in the field of computer vision. The journal article asserts that current Handwritten Text Recognition (HTR) methods often lack tools for end users and may struggle with rare scripts that lack labeled data for training and fine tuning. To solve these problems, they developed and compared 4 different computer vision systems, including two cluster methods and two few shot methods. The few shot methods required slightly more user effort but resulted in higher performance, scalability, and hardware needs. Clustering involves assuming all elements are in one cluster then dividing that cluster into sub clusters until the minimum number of symbols in each cluster is reached. The best clusters are used to identify which symbol matches each cluster best. In the few shot method, each symbol is seen as a node in a graph and the similarity between each pair of symbols is measured. Comparing similarities between symbols can be scaled to compare whole lines Several parts of this paper are applicable to my wider research goals. First, the images used were preprocessed, something I will likely need to do, using binarization, which separates the foreground and background, and segmentation, which removes margins. Furthermore, segmentation is implemented to identify connected components to isolate symbols. I may need to implement these steps to transcribe lines of text. Second, the clustering and few shot methods of computer vision could provide an alternative if the standard approaches fail to work for a specific script. These could also be easier methods to use if I ever create my own data sets because they require little learning. Finally, few shot detection is good for cursive because it avoids segmentation issues.}
}

@misc{modelContextProtocol/getting-started/intro,
  title    = {What is the Model Context Protocol (MCP)?},
  url      = {https://modelcontextprotocol.io/docs/getting-started/intro},
  year     = {2025},
  keywords = {Model Context Protocol, MCP, AI model}
}

@article{cohen_afshar_tapson_schaik_2017,
  title   = {EMNIST: Extending MNIST to handwritten letters},
  doi     = {10.1109/ijcnn.2017.7966217},
  journal = {2017 International Joint Conference on Neural Networks (IJCNN)},
  author  = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Schaik, Andre Van},
  year    = {2017},
  url     = {https://arxiv.org/abs/1702.05373},
  annote  = {This article for the 2017 International Joint Conference on Neural Networks was written primarily by faculty and lecturers at Western Sydney University. Professor Andre van Schaik has a Ph.D. degree in Electrical Engineering, Dr Saeed Afshar is a senior lecturer with degrees in engineering, Dr. has degrees in electrical and computer engineering and Ph.D.’s in Bioelectronics and Neuroscience and Neuromorphic engineering all with the International Centre for Neuromorphic Systems. Additionally, Professor Jonathan Tapson with a Ph.D. in Electrical Engineering, worked at the MARCS Institute for Brain, Behavior, and Development.  These researchers focused on neuromorphic computing or an approach to computing inspired by the human brain, closely tied to AI. They created a dataset from the NIST dataset which used the same image structures as the MNIST dataset. This dataset is called the Extended Modified NIST or EMNIST. For context, the MNIST dataset is a set of handwritten numbers used by Ying Yu and Yuhe Tian in their article “Research Application of Computer Vision-Based Convolution Neural Network in Handwriting Recognition Technology” to develop a computer vision system. This dataset took other elements from the NIST dataset to also include alphabetical characters. Notably, this dataset contains many different writers’ handwriting. This was done to provide an accessible basic dataset for testing purposes. Because of this, it was programmed with splits for testing and has data for validation. Although this dataset was meant for research, I believe that through the division of elements within lines of text it could be used as a basis for a modern handwriting computer vision system. By using this system, I would be able to interact with the datasets using any code that can interact with the very popular MNIST datasets.}
}