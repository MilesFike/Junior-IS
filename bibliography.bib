% Here is an example of how to create a bibliography entry for an article using
% BibTeX. Generally you won't have to write these out yourself, because they are
% provided by most web sites that allow you to export citations. The string
% "clrsAlgorithms" is a citation key, and if you were citing the source in a
% document you would use \cite{clrsAlgorithms}.
@inproceedings{10.1145/3727648.3727679,
  author    = {Yu, Ying and Tian, Yuhe},
  title     = {Research Application of Computer Vision-Based Convolutional Neural Network in Handwriting Recognition Technology},
  year      = {2025},
  isbn      = {9798400712647},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3727648.3727679},
  doi       = {10.1145/3727648.3727679},
  abstract  = {With the rapid development of artificial intelligence technology, handwriting recognition technology plays an important role in many fields. In this paper, the handwriting recognition technology based on convolutional neural network is studied in depth, the basic structure of convolutional neural network is introduced, the construction process of handwriting recognition system is described, and the effectiveness and superiority of the technology is verified through model training and experiments.},
  booktitle = {Proceedings of the 4th International Conference on Computer, Artificial Intelligence and Control Engineering},
  pages     = {177–181},
  numpages  = {5},
  keywords  = {Artificial Intelligence, Computer Vision, Convolutional Neural Networks, Handwriting Recognition},
  series    = {CAICE '25},
  annote    = {This paper analyzes how handwriting recognition technology based on convolution neural networks work. A convolution neural network is generally used for processing grid data and is a feed forward neural network. A convolutional neural network has an input layer that takes in data like 2D images, a convolution layer, which applies kernels to an image, which extract specific features to produce an output feature map, the pooling layer which is used to compress the data and prevent overfitting by taking the averages or maximums of regions, and next, the fully connected layer is used in the last layers of the network and perform tasks like classification on each feature, and finally, the output layer returns results. Handwriting recognition technology is used to convert handwriting into a computer-processable form. This handwriting recognition technology is more widely implemented now because of AI's development and used for diverse purposes including the preservation and retrieval of ancient books. The MNIST is a data set containing handwritten digits. Pytorch was used to build a handwriting recognition model with the MNIST as its training set. The resulting model was vastly successful, but had trouble with multiple digits in succession or with incoherent strokes. The authors of this source, Ying Yu and Yuhe Tian are both associated with the College of Design and Art Shenyang Architecture University, and Yuhe Tian has researched AI in the past, but it is difficult to determine the extent of their background in AI. The sources that the authors use were written by research scholars and faculty at other universities. Overall, the source seems reputable, but the authors are not easily researchable. This source's overview of computer vision implemented in the context of handwriting provided strong background information for any potential investigation computer vision's implementation in identifying historic ligatures which could prove challenging because the authors implied that the computer vision struggled with numbers in succession and this certainly would apply to letters as well. }
}

@article{10.1145/3687310,
  author     = {Ha, Soonhoi and Jeong, Eunjin},
  title      = {Software Optimization and Design Methodology for Low Power Computer Vision Systems},
  year       = {2024},
  issue_date = {January 2025},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {24},
  number     = {1},
  issn       = {1539-9087},
  url        = {https://doi.org/10.1145/3687310},
  doi        = {10.1145/3687310},
  abstract   = {This tutorial article addresses a low power computer vision system as an example of a growing application domain of neural networks, exploring various technologies developed to enhance accuracy within the resource and performance constraints imposed by the hardware platform. Focused on a given hardware platform and network model, software optimization techniques, including pruning, quantization, low-rank approximation, and parallelization, aim to satisfy resource and performance constraints while minimizing accuracy loss. Due to the interdependence of model compression approaches, their systematic application is crucial, as evidenced by winning solutions in the Lower Power Image Recognition Challenge (LPIRC) of 2017 and 2018. Recognizing the typical heterogeneity of processing elements in contemporary hardware platforms, the effective utilization through parallelizing neural networks emerges as increasingly vital for performance enhancement. The article advocates for a more impactful strategy—designing a network architecture tailored to a specific hardware platform. For detailed information on each technique, the article provides corresponding references.},
  journal    = {ACM Trans. Embed. Comput. Syst.},
  month      = dec,
  articleno  = {19},
  numpages   = {31},
  keywords   = {Optimization, neural architecture search, parallelization, embedded machine learning},
  annote     = {This article written by Soonhoi Ha, a professor of Computer Science and Engineering, and Eunjin Jeong, a post-doctoral researcher in the same field, in 2024 discusses computer vision systems designed both for accuracy and system constraints. It encourages the use of specialized hardware to optimize computer vision systems with deep learning. They suggest reducing redundancy in deep learning with approximate computing that does not cause significant accuracy loss. Examples in this article demonstrate systems with several convolutional layers followed by a few fully connected layers. The article goes on to discuss the results of implementing different types of quantization, quantization being the reduction of bits in representation, pruning of unnecessary parameters, reduction of large-size kernels to reduce storage requirements and computation time, and other optimization techniques. The article also strongly emphasizes when different types of hardware such as GPUs can be used to process layers. This article has been downloaded hundreds of times and continues to be downloaded frequently but it has no recorded citations on the ACM Digital Library website. The sources the article references tend to originate from scientific journals and conferences and appear credible. The article provides specific code examples for how the elements of computer vision can be programmed which would be very applicable to work in the field, and it reinforces the understanding of the basic concepts of layers in computer vision by showing examples of their implementations.}
}

@inproceedings{10.1145/3544548.3580643,
  author    = {Gyory, Peter and Bae, S. Sandra and Yang, Ruhan and Do, Ellen Yi-Luen and Zheng, Clement},
  title     = {Marking Material Interactions with Computer Vision},
  year      = {2023},
  isbn      = {9781450394215},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3544548.3580643},
  doi       = {10.1145/3544548.3580643},
  abstract  = {The electronics-centered approach to physical computing presents challenges when designers build tangible interactive systems due to its inherent emphasis on circuitry and electronic components. To explore an alternative physical computing approach we have developed a computer vision (CV) based system that uses a webcam, computer, and printed fiducial markers to create functional tangible interfaces. Through a series of design studios, we probed how designers build tangible interfaces with this CV-driven approach. In this paper, we apply the annotated portfolio method to reflect on the fifteen outcomes from these studios. We observed that CV markers offer versatile materiality for tangible interactions, afford the use of democratic materials for interface construction, and engage designers in embodied debugging with their own vision as a proxy for CV. By sharing our insights, we inform other designers and educators who seek alternative ways to facilitate physical computing and tangible interaction design.},
  booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  articleno = {478},
  numpages  = {17},
  keywords  = {Computer Vision, Making, Materiality, Physical Computing, Tangible Interactions},
  location  = {Hamburg, Germany},
  series    = {CHI '23'},
  annote    = {This paper, written in part by the faculty of the ATLAS Institute at the University of Colorado Boulder in 2023, describes a computer vision system that can be run on a local device with a webcam. This is especially helpful as previous articles have been dependent on hardware that may not be accessible to all. This source has 5 citations showing higher engagement than the other sources investigated. The researchers in this paper investigated the creation of a Tangible User Interface system relying on computer vision rather than electronics to receive input due to the struggles novices have with devices like Arduino. They developed the beholder JavaScript library which gives users control over a device’s cameras. Beholder could then use the cameras to read ArUco markers and acquire their metadata. ArUco markers resemble QR codes. After the library was designed, it was used in the projects of students. They would create systems and physical devices for which the ArUco marker could be read by Beholder, like a little arcade machine which moved the ArUco marker when a button was pressed. The system was easy to debug because visual information could indicate issues. Limitations of the system include proper lighting and space requirements for the camera. Aside from reinforcing a basic understanding of computer vision, this source is not the most relevant. However, this article does encourage consideration of elements like perspective and lightning when considering what a computer may be required to interpret.}
}

@article{10.1145/3754333,
  author    = {Ponte Ah\'{o}n, Santiago and Aidelman, Yael and Seery, Juan and Quiroga, Facundo Manuel and Ronchetti, Franco and Hasperu\'{e}, Waldo and Iannuzzi, Matilde and Peralta, Romina and Lopez, M\'{o}nica and Bariviera, Aurelio F. and Cidale, Lydia and Gamen, Roberto},
  title     = {ReTrOH-UNLP: Conservation of the Historical Observational Work of the Astronomical Observatory of La Plata with Computer Vision},
  year      = {2025},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  issn      = {1556-4673},
  url       = {https://doi.org/10.1145/3754333},
  doi       = {10.1145/3754333},
  abstract  = {The Observatorio Astron\'{o}mico de La Plata from the Facultad de Ciencias Astron\'{o}micas y Geof\'{\i}sicas (FCAG) of the Universidad Nacional de La Plata (UNLP) is one of the oldest observatories in South America. It has an enormous collection of observations, including thousands of spectroscopic and photographic glass plates. These observations were taken between the 1900s and the 1980s by renowned Argentine astronomers and constitute unique and unpublished records.Since 2019, the Recuperation of Historical Observational Work (ReTrOH, for its initials in Spanish) project has begun digitizing the 15,000+ samples of the spectroscopic plate collection.The digitization process for each plate is a complex, error-prone, multi-stage procedure requiring several person-hours. Consequently, a multidisciplinary team of astronomers and computer scientists was formed to develop the first system capable of assisting the user in this process. This system, designated PlateUNLP, employs signal processing algorithms and computer vision models to accelerate the process, reduce error rates, and standardizes data formats and processes. A key feature of PlateUNLP is its ability to automatically detect and record each spectrum recorded on the plates, along with the corresponding metadata, thus minimizing the need for user intervention.The developed system is publicly available under an open source license. Furthermore, while the system was developed with the ReTrOH project's requirements in mind, it could be adapted and used to assist with the digitization of thousands of other plates available around the world.PlateUNLP is therefore a first step towards making possible and practical the recovery of historical astronomical observations. Digitizing and exploiting these observations can provide a unique window into the past of our universe.},
  note      = {Just Accepted},
  journal   = {J. Comput. Cult. Herit.},
  month     = jul,
  keywords  = {Astronomy, Spectrographic records, Spectroscopic plates, Computer Vision, YOLO, Object Detection, Wavelength calibration, Dynamic Time Warping}
}

